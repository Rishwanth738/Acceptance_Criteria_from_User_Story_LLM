# -*- coding: utf-8 -*-
"""JSON.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18cJjBNw8ERrRJ0x6mgQd53aEzG581Lzm
"""

# STEP 1: Same as before
!pip install requests tqdm --quiet

import requests
import json
import time
from tqdm import tqdm
from google.colab import files

API_KEY = "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"  # ðŸ” Replace this

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

# Upload the JSON
uploaded = files.upload()
input_file = list(uploaded.keys())[0]

with open(input_file, 'r') as f:
    data = json.load(f)

# --- Chunk helper ---
def chunk_list(lst, size):
    for i in range(0, len(lst), size):
        yield lst[i:i + size]

# --- Prompt builder ---
def generate_payload(user_story, criteria_chunk):
    prompt = f"""
User Story:
{user_story}

Positive Acceptance Criteria:
{chr(10).join([f"- {c}" for c in criteria_chunk])}

TASKS:
1. Assign a ROLE to each criterion: User, System, or Both.
2. Generate 2 NEGATIVE acceptance criteria for invalid inputs or errors.
3. Please ONLY respond with valid JSON. Do not include any commentary.

Return JSON like:
{{
  "positive_criteria": [{{"criterion": "...", "role": "..."}}],
  "negative_criteria": [{{"criterion": "...", "role": "..."}}]
}}
"""
    return {
        "model": "mistralai/mixtral-8x7b-instruct",
        "messages": [
            {"role": "system", "content": "You are a QA engineer helping label acceptance criteria."},
            {"role": "user", "content": prompt.strip()}
        ]
    }

# --- Main enrichment loop ---
final_output = []

for item in tqdm(data, desc="Processing user stories"):
    story = item["user_story"]
    all_criteria = item["positive_criteria"]

    all_positive = []
    all_negative = []

    for chunk in chunk_list(all_criteria, 2):  # Chunk in 2s
        payload = generate_payload(story, chunk)

        try:
            res = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=HEADERS, json=payload)
            result = res.json()

            if "choices" not in result:
                print(f"âŒ Skipped chunk for story: {story[:60]}")
                print("Response:", json.dumps(result, indent=2))
                continue

            content = result["choices"][0]["message"]["content"]
            parsed_json = json.loads(content)

            all_positive.extend(parsed_json.get("positive_criteria", []))
            all_negative.extend(parsed_json.get("negative_criteria", []))

            time.sleep(1.2)

        except Exception as e:
            print(f"ðŸ’¥ Error for chunk in: {story[:60]} => {e}")
            continue

    final_output.append({
        "user_story": story,
        "positive_criteria": all_positive,
        "negative_criteria": all_negative
    })

# Save final file
with open("enriched_user_stories.json", "w") as f:
    json.dump(final_output, f, indent=2)

files.download("enriched_user_stories.json")
print("âœ… Done! Full enrichment with batching complete.")

# STEP 1: Install dependencies
!pip install requests tqdm --quiet

# STEP 2: Imports
import requests
import json
import time
from tqdm import tqdm
from google.colab import files

# STEP 3: Add your OpenRouter API key
API_KEY = "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"  # ðŸ” Replace this with your real key

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

# STEP 4: Upload your parsed_user_stories JSON file
uploaded = files.upload()
input_file = list(uploaded.keys())[0]

with open(input_file, 'r') as f:
    data = json.load(f)

# STEP 5: Chunk helper
def chunk_list(lst, size):
    for i in range(0, len(lst), size):
        yield lst[i:i + size]

# STEP 6: Prompt builder with JSON enforcement
def generate_payload(user_story, criteria_chunk):
    prompt = f"""
User Story:
{user_story}

Positive Acceptance Criteria:
{chr(10).join([f"- {c}" for c in criteria_chunk])}

TASKS:
1. Assign a ROLE to each criterion: User, System, or Both.
2. Generate 2 NEGATIVE acceptance criteria that describe invalid inputs or errors.
3. Please ONLY respond with valid JSON. Do not include any explanation or commentary.

Format like:
{{
  "positive_criteria": [{{"criterion": "...", "role": "..."}}],
  "negative_criteria": [{{"criterion": "...", "role": "..."}}]
}}
"""
    return {
        "model": "mistralai/mixtral-8x7b-instruct",
        "messages": [
            {
                "role": "system",
                "content": "You are a QA engineer helping write acceptance criteria."
            },
            {
                "role": "user",
                "content": prompt.strip()
            }
        ]
    }

# STEP 7: Enrichment loop with safe JSON parsing
final_output = []

for item in tqdm(data, desc="Processing user stories"):
    story = item["user_story"]
    all_criteria = item["positive_criteria"]

    all_positive = []
    all_negative = []

    for chunk in chunk_list(all_criteria, 2):  # Batch in 2s
        payload = generate_payload(story, chunk)

        try:
            res = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=HEADERS, json=payload)
            result = res.json()

            if "choices" not in result:
                print(f"âŒ API error for chunk in: {story[:60]}")
                print(json.dumps(result, indent=2))
                continue

            content = result["choices"][0]["message"]["content"].strip()

            # Quick check for valid JSON structure
            if not content.startswith("{"):
                raise ValueError(f"Invalid JSON response start: {repr(content[:50])}")

            parsed_json = json.loads(content)

            all_positive.extend(parsed_json.get("positive_criteria", []))
            all_negative.extend(parsed_json.get("negative_criteria", []))

            time.sleep(1.2)  # Avoid hammering the API

        except Exception as e:
            print(f"ðŸ’¥ Error for chunk in: {story[:60]} => {e}")
            print("Raw GPT content:", repr(content if 'content' in locals() else 'No content returned'))
            continue

    final_output.append({
        "user_story": story,
        "positive_criteria": all_positive,
        "negative_criteria": all_negative
    })

# STEP 8: Save enriched output
with open("enriched_user_stories.json", "w") as f:
    json.dump(final_output, f, indent=2)

files.download("enriched_user_stories.json")
print("âœ… Done! Download enriched_user_stories.json")

!pip install -q transformers datasets peft accelerate trl bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import torch
import json

data = []
with open("/content/qa_finetune_data.jsonl", "r") as f:
    for line in f:
        row = json.loads(line)
        data.append({
            "prompt": row["prompt"],
            "completion": row["completion"]
        })

dataset = Dataset.from_list(data)

model_id = "mistralai/Mistral-7B-Instruct-v0.3"

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    device_map="auto",
    trust_remote_code=True
)

from huggingface_hub import notebook_login

notebook_login()

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

training_args = TrainingArguments(
    output_dir="./mistral-qa-lora",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=20,
    num_train_epochs = 7,
    learning_rate=2e-4,
    bf16=False,
    fp16=True,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    lr_scheduler_type = "cosine",
    load_best_model_at_end=True,
    logging_dir="./logs"
)

from trl import SFTTrainer

# âœ… Correct formatter for datasets.map()
def formatting(example):
    return {"text": f"{example['prompt']}\n{example['completion']}"}

# âœ… Format the dataset beforehand
formatted_dataset = dataset.map(formatting)

eval_dataset = formatted_dataset.select(range(50))  # optional small slice

trainer = SFTTrainer(
    model=model,
    train_dataset=formatted_dataset,
    eval_dataset=eval_dataset,  # optional
    args=training_args,
)

formatted_dataset[0]

trainer.train()

model.save_pretrained("mistral-qa-lora")
tokenizer.save_pretrained("mistral-qa-lora")

input_text = """User Story:
As a customer, I want to receive payment confirmation via SMS, so I can verify successful transactions instantly.

Task: Generate positive and negative acceptance criteria as structured JSON."""

inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

output = model.generate(
    **inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

print(tokenizer.decode(output[0], skip_special_tokens=True))

import shutil
from google.colab import files

# Zip the folder
shutil.make_archive("mistral-qa-lora", 'zip', "mistral-qa-lora")

# Download it
files.download("mistral-qa-lora.zip")

